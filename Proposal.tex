%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 12pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images
\usepackage{hyperref}

\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}
\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\lhead{Master Thesis Proposal}
%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Applications of Scalable Machine Learning Models}\\ % Title
} % Subtitle

\author{\textsc{Mingzhe Xu} % Author
\\{\textit{Northeastern University}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section
\tableofcontents
%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else

%\begin{abstract}
%Advertisement ranking is of great value to research given the enormous revenue it generates in industry. Predicting click through rate is not a brand new topic. Yet new methodologies and approaches to tackle it as well as improve the accuracy confer tremendous vitality to this subject. This thesis will focus on predicting the click through rate (CTR) of ads in a web search engine SOSO. 
%\end{abstract}

% \hspace*{3,6mm}\textit{Keywords:}  % Keywords

\vspace{30pt} % Some vertical space between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section{Introduction}
Machine learning is a method to enable computers to learn without explicitly being programmed\cite{mldef}. In the late 1990s, computer scientists started to train computers to build models given the advance of digitalization and cheaper computing power. With the vast volume and increasing complexity of data, as well as accessible and affordable storage in the past decades, interests in machine learning surged unprecedentedly and applications became almost ubiquitous, which make it possible to quickly and automatically produce models that can analyze complex data and deliver faster, more accurate results. Thomas H. Davenport once wrote in The Wall Street Journal that "humans can typically create one or two good models a week; machine learning can create thousands of models a week"\cite{wsj}. Precisely. Machine learning instantaneously presents high-value predictions that can guide better decisions and smarter actions in real time without any human intervention. 

\subsection{Applications of Machine Learning}

The world now is swimming in machine learning products more than ever. In a variety of industries and analytical settings where access to troves of useful and reliable data is provided, machine learning is being frequently applied to explore useful information, generate accurate prediction, and directly or indirectly increase revenues. Online retailers can recommend products closely related to customers' interests. Amazon personalized recommendations, which base on customers' order history, items in carts, items rated or liked, and products similar customers viewed and purchased, provide heavily customized browsing experience for users. This personalized marketing has generated lots of revenue. According to a survey conducted in 2015, 68.4\% of all revenue of the online retailer was from the recommendation system\cite{pr-revenue}. Netflix also provide individualized recommendations that greatly improved user experience and more than half of the viewing is from recommendation. Similarly, YouTube's Watch Next is also an embodiment of personalized recommendation. What's more, machine learning also pops up in web search results. Search engine is the lifeblood to Google, and personalized search results as well as customized ads display are core to the click through rate of the results. They are based on the combinations of queries, ads, results, and users' browsing history. In addition to Google, Facebook's News Feed, LinkedIn's Posts also ranks in a customized order. In banking system, credit scoring, fraud detection and next-best offers are all built on machine-learning algorithms\cite{bank}. The recent popular computer program AlphaGo beats professional human Go players after learning from a database of around 30 million moves\cite{alphago}. In speech and image recognition, machine-learning techniques are also widely used. \\

In the meantime, there are constraints. With the exceptional increase in computing power, storage capacity and network bandwidth in the past decade, datasets are growing fast in fields such as search engine, bioinformatics, IT-security, speech/image recognition, or email record, to name but a few. The growth of data size leaves computational methods the only viable way of dealing with data. However, it poses new challenges to machine learning methods. Google processes around 100 PB per day on 3 million servers; YouTube has 300PB storage and around 4 billion views per day\cite{youtube}. The Big Data industry roughly process 2.5 quintillion bytes of data every day\cite{bigdata}. And the number is not still. Looking forward, in four years there will be 40 zetta-bytes of data. Therefore, how to make the learning process more efficient is vital to those companies. With at least 3.5 billion requests coming in everyday, what algorithms can make faster and more accurate response? What models can survive the training process on petabytes of data? 

\subsection{Scalability of Machine Learning}
This explosion of data tuned widespread attention to scalability, especially learning large-scale data. Scalable machine learning involves processing paradigms, statistical analysis, algorithms for data streams, and large scale convex optimization, etc. It is an integration of system, statistics, data mining and machine learning. Scalability is one of the key concepts in Big Data. Yet far beyond Big Data, scalable or large scale was a buzzword in machine learning ever since there were large amount of data such as dealing with text document or in bioinformatics. Datasets with large number of features, samples, or even when the data can't fit into the memory, solutions are needed to enable learning and processing. Currently, there are some solutions on different levels. In memory, we can intellectually swap between memory or disk. On the algorithmic level, we can use online algorithm which can constantly learn and predict as the data flows\cite{onlinelearn}. Also, faster and optimized algorithms are needed. Parallel algorithms are vital to higher efficiency. When data cannot fit into memory of a single machine, distributed algorithms can tackle this problem. 

\subsection{Thesis Objectives}
There are two core purposes for this thesis course. First is learning and studying scalable machine learning theories. Laying a firm theoretical foundation for conducting practical experiments on large scale machine learning is crucial. The second purpose is to apply the concepts into solving real world problems with large data so as to put the theories learned into practice. 
%------------------------------------------------
\section{Theoretical Foundation}
Scalable machine learning is constructed with several key subjects. First is machine learning models that can be scalable. Second, efficient learning algorithms are key to a better prediction, with which the performance of the model will be much better. We also learned the model selection evaluation so as to better gauge the model's predicting power and select the best model. Last but not least, we covered feature design and feature selection.
\subsection{Scalable ML Models}
In this semester, we've gone through several fundamental learning algorithms, as well as objective functions and models. Supervised learning problems are those with input features, such as linear regression and classification problems. Linear regression is used when the target variable to be predicted is continuous; when the prediction are discrete values such as predicting positive or negative of a certain disease, it is a classification problem. To minimize the cost functions, Least Mean Square algorithm, matrix derivatives or maximum likelihood estimate can be applied to derive the parameters for models\cite{Linear}. Logistic regression models assumes there is one smooth linear decision boundary and it is a powerful statistical way of modeling when the dependent variable is a binomial value. It is also the most prevalent algorithm for solving the scale problem in industry given its simplicity, efficiency and robustness. Naive Bayes is a family of algorithms for training such as Gaussian naive Bayes, Bernoulli, or multinomial naive Bayes, assuming that all values of variables are independent of other features\cite{nb, nb2}. Modeling with Naive Bayes can be simple and efficient while the conditions can be the constraints.\\

Supporting Vector Machine (SVM) relies on boundary cases to build the separating curves and obtain the optimal margin classifier. SVM can handle large feature spaces and the boundary cases also enable it to handle missing data such in text classification\cite{svm, svm2}. We've also discussed about decision tree, or regression tree, which is derived by recursively partition the data space and fitting prediction model within each partition\cite{decisiontree, decisiontree2, decisiontree3, decisiontree4}. It is more intuitive and easy to interpret given the tree structure. While the complexity of the algorithm is it's main setback. In order to improve the classification rate, we can also use random forest, which applies a number of decision trees\cite{rf}.\\

Aside from these basic modeling methods, we are also going to try ensemble modeling such as AdaBoost, Deep Neural Network and Markov Chain Monte Carlo (MCMC). AdaBoost is a meta-algorithm which is used in conjunction with other learning algorithms to improve performances\cite{adaboost}. The training process is efficient since it picks features which improve the predictive power of the model based on the weight and intermediate result for each round. Therefore, it reduces dimensionality and  improves the efficiency of learning as less relevant features are not being evaluated. Deep Neural Network(DNN) is also a scalable model and promising to try in practice. DNN is good with modeling complex non-linear relationships\cite{dnn, dnn1}. Also, since it has multiple hidden layers of units between the input and output layers, it enables the composition of features from lower layers, making it possible to model complex data with fewer units.
\subsection{Efficient Learning Algorithms}
Convex optimization [gradient based algorithm, Newton]
Parallel. [stochastic] gradient descent/ascent
\subsection{Model Selection Evaluation}
Train, CV, Test
     2. Eval metrics [MSE, RMSE, AbsError/L1, ROC, F1 metrics, etc.]
     3. Regularization
\subsection{Feature Design/Selection}
1. PCA, SVD, Maximum Mutual Information [selection]
     2. [design]

\section{Experiments with Scalable Machine Learning}
After laying a theoretical foundation, I am going to design and experiment large-scale modeling techniques on the dataset from KDD Cup 2012 track 2, which is on advertisement ranking. Clicking ads prediction is of great value to research given the enormous revenue it generates in industry. Predicting click through rate (CTR) is central to multi-billion dollar business in Internet marketing. Contextual advertising, display advertising and search advertising all depend heavily on an efficient learning model that can accurately predict the click through rate\cite{ctr, ctr2, ctr3, ctr4, ctr5, ctr6}. Predicting click through rate is not a brand new topic. Yet new methodologies and approaches to tackle it as well as improve the accuracy confer tremendous vitality to this subject. This thesis will focus on applying large-scale learning techniques and predicting the CTR of ads in a web search engine. 

\subsection{Problem Definition/Description}
Search advertising has been one of the major revenue sources of the Internet industry for years. A key technology behind search advertising is to predict the click-through rate (pCTR) of ads, as the economic model behind search advertising requires pCTR values to rank ads and to price clicks. In this task, given the training instances derived from session logs of the Tencent proprietary search engine, soso.com, participants are expected to accurately predict the pCTR of ads in the testing instances.
\subsection{Data}
\begin{table}[ht]
\centering
\caption{Data File Description}
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
\textbf{File}  & \textbf{Size} & \textbf{Records} \\ \hline
training.txt	 & 9.9GB	&149,639,105 \\ \hline
test.txt	&1.3GB	&20,297,594 \\ \hline
KDD\_Track2\_solution.csv	& 244MB	&20,297,595\\ \hline
descriptionid\_tokensid.txt &	268MB	& 3,171,830 \\ \hline
purchasedkeywordid\_tokensid.txt &	26MB	&1,249,785 \\ \hline
queryid\_tokensid.txt	 & 704MB & 	26,243,606 \\ \hline
titleid\_tokensid.txt	& 171MB	& 4,051,441 \\ \hline
serid\_profile.txt& 	283MB	&23,669,283 \\ \hline
\end{tabular}
\end{table}

The dataset is provided by Tencent and from the competition in track 2 of KDD Cup 2012. It includes training data, testing data and other additional supporting datasets. Training data contains149, 639,105 instances while the testing data 20,297,594 instances. They both capture the data between user and search engine, including some shared features such as UserID, DisplayUrl, KeywordID, QueryID, AdID, AdvertiserID, DescriptionID, Position and Depth. In addition, the training data also has impression and click whereas the testing data does not. For each instance, the meaning is that in a case, the user is impressed with the ad for certain number of impressions and clicked the ad number of click times. The supporting data contains token lists of query, description and keyword.\\

\begin{table}[ht]
\centering
\caption{Data Field Description}
\label{my-label}
\begin{tabular}{|l|l|}
\hline
\textbf{Field Name}  & \textbf{Field Description}  \\ \hline
UserID 			& id  for user\\ \hline
AdID                	 	& certain ad users impress or click   \\ \hline
AdvertiserID       	 & advertiser for ad, a property of ad   \\ \hline
Depth 	   		 & number of ads impressed in a session \\ \hline
Position		   	& the order of an ad in the impression list \\\hline
KeywordID		& the keyword of ad, a property of ad \\
				& it is the key of keyword token file\\ \hline
QueryID		 	& id of the query \\
				& zero-based integer value, key of query token file\\ \hline
DisplayURL		& a property of the ads (hashed for anonymity) \\ \hline
TitleID			&  a property of ads, key in title token file \\ \hline
DescriptionID      	 &  a property of ads, key in description token file \\ \hline
Click				&  number of times the user clicked the ad\\ \hline
Impression 		& number of search sessions in which \\ 
				& the ad was impressed by the user	\\ \hline
\end{tabular}
\end{table}

The training data contains impression and click, where impression is the number of times the ad was impressed by the user, and click is the number of times the impression transferred into an actual click. For every impression, the user may click or not click the ad, which can be viewed as a binary classification.  In the original dataset, the click and impression are not binary-based values. Therefore a modification was made to accommodate binary classification. Each data instance was split into two with equivalent embodiment of the original CTR. For example, an instance with 4 clicks and 10 impressions will be split into 1 click with weight 4 and 0 click with weight 6. Regarding the features in the original dataset, modifications also made to have better prediction model. Each feature is represented by the corresponding value such as UserID, QueryID etc., and based these features new CTR features can be generated. For each categorical feature, we first group them and get the average click through rate for each category, then put the value back correspondingly to the original dataset as an additional column of feature. 

%------------------------------------------------
\subsection{Evaluation Criteria}
The solution will be validated by area under curve (AUC) of the receiver operating characteristic (ROC) curve. ROC curve depicts the classification performance in a two dimensional graph with false positive (FPR) rate and true positive (TPR) rate. Calculating the area under the ROC curve gives us the probability that the classifier ranks a randomly chosen positive instance higher than a randomly chosen negative instance\cite{roc}. Since ROC is in a unit space, AUC is a value between 0 and 1. In the coordinate, the best prediction method will yield a prediction on the upper left, (0, 1), which means that there is no false positive. This measurement is well known for ranking performance and classification performance. Though arguments exist on the coherency of AUC in measuring aggregated classifier performance, the proposed alternative was proved to be already available in the form of AUC\cite{roc2, roc3}.\\
\includegraphics[width=0.5\textwidth]{/Users/mingzhexu/Desktop/maggie_xu_thesis/proposal/ROC_space.png} \\Figure. ROC Space (Wikipedia)\\

\subsection{Preliminary Results}
Progress has been made on both data pre-processing and some initial modeling. The original dataset contains data on user and search engine, while their values can not be directly used in models. Therefore new CTR features were generated based on the original features. For the dependent variable, which is click, changes were made as well. The raw data contains click value ranging from 0 up to thousands, and the corresponding impression data is an integer larger than click. However, I would like to treat the clicking event as a binary value, either click or not click. Therefore, I made a change to the data on the impression and click. Based on the impression, if it is larger than 1, then the data instance will be split based on how many clicks it has. If the number of clicks is equal to impression, then the click will be updated to 1. If the number of clicks is 0 then no modification is needed. Otherwise, the instance will be split into to two instances. The clicks will be 1 and 0 respectively and impressions for 1 will be the original number of clicks, while impression for 0 will be the  original number of impressions minus the original number of clicks. Other than the given features in the original data, new features will be generated from the token files as well. For features such as query, title and description, the token strings will be stemmed using Snowball stemmer. What's more, term frequency-inverse document frequency (tf-idf) will be used for user's query and ads' description\cite{tf-idf}.\\

In addition to feature modification, the original data was downsized into a 5 million instances sample in order to process faster locally. The original data contains 150 million data instances with 10gb raw train data as well as token data, user profiles data. Downsizing the data into 5 million instance sample is a valid approach for this stage of proof of concept. On one side, it is faster and more efficient for model selection and feature selection where datasets need to be run multiple times so as to get the best score for features and models. On the other side, 5 million is a sufficient data sample size for initial processing of data and catch the gist for different sets of features and models.\\

Regarding trying different models, logistic regression model, based on the theoretical view, is efficient for large scale learning and good for its robustness to overfitting. I first tried this model on all the features I have for now. With 5 million instances, I split the data into two parts, training and testing, 70\% and 30\% respectively for cross validation. Here is the preliminary result after running on my sampled data.\\
\includegraphics[width=0.8\textwidth]{/Users/mingzhexu/Desktop/maggie_xu_thesis/proposal/ROC.jpg}\\
The AUC score is 0.84 which is much higher than the given benchmarks. However, due to the fact that the datasets I adopted are sampled from the original data, the variance is higher so this score might be a little bit higher than fitting the model on the full data, which contains a test file with more than 20 million instances.

%------------------------------------------------
\subsection{Future Work}
In next semester, I am planning to learn more advanced scalable machine learning algorithms, models and techniques. In the meantime, this project will be developed into a fully-fledged thesis paper. In order to achieve that, I need to first keep taking this thesis course and study more on the related topics to hone skills in this field. Besides, here is a more detailed plan for my thesis:

First, new features are needed to be explored from the original data. Stemming the tokens, I can then pick valuable information and construct new ads related features from description and keywords, as well as user related feature from query tokens.

Second, model selection needs to be updated given that the new features popping up. Also, with more thorough study on some advanced models, I can try ensemble modeling or feature engineered features such as AdaBoost, Random Forest, SVM, Neural Network etc. 

Third, after finding the model with top AUC scores on my sample data, I will need to run the models on the full data, which contains 149,639,105 instances. Due to the size of the datasets and the constraints of my local machine, it is hard to run models locally. So, additional tasks required to mount the data to a Cloud on Amazon Web Service, Google Cloud or Azure.\\
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
%\bibliographystyle{unsrt}
\begin{thebibliography}{9}
\bibitem{decisiontree4}
Breiman, L., J. Friedman, and R. Olshen (1984). Classification and Regression trees. Wadsworth. 555 - 597
\bibitem{bank}
Chan, Philip K., and Salvatore J. Stolfo. "Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection." KDD. Vol. 1998.
\bibitem{Linear} 
Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining, Linear Regression Analysis, page 67-119, Edition 5, 2012
\bibitem{nb2}
Drugowitsch, J. (2008). Bayesian Linear Regression. Technical report, U.Rochester.
\bibitem{ctr}
D. Agarwal, B.-C. Chen, and P. Elango. Spatio-temporal models for estimating click-through rate. In Proceedings of the 18th international conference on World wide web, pages 21-30. ACM, 2009
\bibitem{onlinelearn}
Duchi, J., E. Hazan, and Y. Singer (2010), Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Retrieved from \\
\href{url}{http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf}
\bibitem{roc}
Fawcett, Tom (2006); An introduction to ROC analysis, Pattern Recognition Letters, 27, 861-874. Retrieved from\\
\href{url}{https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf}
\bibitem{roc2}
Hand, D.J. Measuring classifier performance: a coherent alternative to the area under the ROC curve. Machine learning, 77(1): 103-123, 2009. ISSN 0885-6125.
\bibitem{roc3}
Hanley, James A.; McNeil, Barbara J. (1983). "A method of comparing the areas under receiver operating characteristic curves derived from the same cases". Radiology 148 (3): 839-843. 
\bibitem{decisiontree2}
 Kim H, Loh WY. Classification trees with bivariate linear discriminant node models. J Computer Graphical Stat 2003, 12:512-530.
 \bibitem{decisiontree3}
 Kim H, Loh WY, Shih YS, Chaudhuri P. Visu- alizable and interpretable regression models with good prediction power. IIE Trans 2007, 39:565-579.
 \bibitem{rf}
 Liaw, Andy, and Matthew Wiener. "Classification and regression by randomForest." R news 2.3 (2002): 18-22.
\bibitem{ctr5}
M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web, pages 521-530. ACM, 2007. 
\bibitem{ctr6}
M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. World Wide Web Conference Series, 2007
\bibitem{nb}
 Rennie, J.; Shih, L.; Teevan, J.; Karger, D. (2003). Tackling the poor assumptions of Naive Bayes classifiers
 \bibitem{svm2}
Stuart Andrews, Ioannis Tsochantaridis and Thomas Hofmann: Support Vector Machines for Multiple-Instance Learning. Retrieved from\\ 
\href{url}{https://www.robots.ox.ac.uk/~vgg/rg/papers/andrews\_etal\_NIPS02.pdf}
\bibitem{svm}
Thorsten Joachims: Text Categorization with Support Vector Machines: Learning with Many Relevant Features,\\
\href{url}{http://www.cs.cornell.edu/people/tj/publications/joachims\_98a.pdf}
\bibitem{decisiontree}
Wei-Yin Loh: Classification and regression trees. Retrieved from\\ 
\href{url}{http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf}
\bibitem{ctr4}
O. Chapelle. Click modeling for display advertising. In AdML: 2012 ICML Workshop on Online Advertising, 2012. 
\bibitem{mldef}
Phil Simon (March 18, 2013). Too Big to Ignore: The Business Case for Big Data. Wiley. p. 89
\bibitem{tf-idf}
Ramos, Juan. "Using tf-idf to determine word relevance in document queries." Proceedings of the first instructional conference on machine learning. 2003.
\bibitem{alphago}
Silver, David, et al. "Mastering the game of Go with deep neural networks and tree search." Nature 529.7587 (2016): 484-489.
\bibitem{ctr2}
T. Graepel, J. Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian click-through rate prediction for sponsored search advertising in microsofts bing search engine. 2010
\bibitem{ctr3}
Z. A. Zhu, W. Chen, T. Minka, C. Zhu, and Z. Chen. A Novel Click Model and its Applications to Online Advertising.\\ http://research.microsoft.com/pubs/119092/WSDM2010.pdf
\bibitem{youtube}
Article retrieved from
\href{url}{http://www.slideshare.net/kmstechnology/big-data-overview-2013-2014}
\bibitem{bigdata}
Article retrieved from
\href{url}{http://cloudtweaks.com/2015/03/surprising-facts-and-stats-about-the-big-data-industry/}
\bibitem{wsj}
Article retrieved from
\href{url}{http://blogs.wsj.com/cio/2013/09/11/industrial-strength-analytics-with-machine-learning/}
\bibitem{pr-revenue}
Article retrieved from
\href{url}{https://www.marketingsherpa.com/article/chart/personalized-product-recommendations}
\bibitem{adaboost}
Yoav Freund , Robert E. Schapire: A Short Introduction to Boosting (1999),\\ retrieved from \href{url}{http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf}
\bibitem{dnn}
Schmidhuber, Jürgen. "Deep learning in neural networks: An overview." Neural Networks 61 (2015): 85-117.
\bibitem{dnn1}
Bengio, Yoshua (2009). "Learning Deep Architectures for AI". Foundations and Trends in Machine Learning 2 (1): 1-127. \\ Retrieved from \href{url}{https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf}
%\bibliography{sample}
\end{thebibliography}
%----------------------------------------------------------------------------------------

\end{document}